{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5\n",
      "  Device 1: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from ../model/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  107.56 MiB\n",
      "llm_load_tensors: VRAM used           = 8694.21 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 6400.00 MiB, K (f16): 3200.00 MiB, V (f16): 3200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 5491.22 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 5488.03 MiB\n",
      "llama_new_context_with_model: total VRAM used: 14182.24 MiB (model: 8694.21 MiB, context: 5488.03 MiB)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# import langchain and llamacpp\n",
    "# llama cpp is a C++ interface that enables LLM to run locally with GPU support\n",
    "# we should initialize a llamacpp instance first and later langchain provides a wrapper for it\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../model/llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    n_ctx=8192, # context of token size\n",
    "    n_gpu_layers=-1, #setting -1 to offload all the LLM layers to all available gpu \n",
    "    n_batch=4096, # no. of token in a prompt fed in LLM each time in a batch\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    max_tokens=2048 # max tokens LLM could generate/answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Langchain prompt messages, e.g., System Message, Human Message to provide context and instruct how LLM could answer our questions\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "# Create a simple prompt for illustration purpose\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the standard chain interface\n",
    "# import memory class to store the chat history\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the specialized wrappers for llama chat models\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure thing! Here are a few popular locations to consider visiting in Vienna:\n",
      "\n",
      "1. Sch√∂nbrunn Palace\n",
      "2. St. Stephen's Cathedral\n",
      "3. Hofburg Palace\n",
      "4. Prater Park\n",
      "5. Belvedere Palace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     992.99 ms\n",
      "llama_print_timings:      sample time =      33.98 ms /    55 runs   (    0.62 ms per token,  1618.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     992.68 ms /    47 tokens (   21.12 ms per token,    47.35 tokens per second)\n",
      "llama_print_timings:        eval time =    4372.23 ms /    54 runs   (   80.97 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    5557.05 ms\n"
     ]
    }
   ],
   "source": [
    "# test to see whether the Llama 2 model works\n",
    "print(\n",
    "    chain.run(\n",
    "        text=\"What can I see in Vienna? Propose a few locations. Names only, no details.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are some details about St. Stephen's Cathedral (Stephansdom) in Vienna, Austria:\n",
      "\n",
      "St. Stephen's Cathedral is a stunning Gothic cathedral located in the heart of Vienna and is one of the city's most famous landmarks. It was built in the 12th century and has been the site of important religious and political events throughout history. The cathedral features intricate stone carvings, colorful tile roofs, and a stunning stained glass window that depicts the life of Saint Stephen. Visitors can climb up to the South Tower for panoramic views of the city.\n",
      "\n",
      "It's also worth noting that the cathedral is located in the historic center of Vienna, so it's a great starting point for exploring the city's other famous landmarks and attractions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     992.99 ms\n",
      "llama_print_timings:      sample time =     116.28 ms /   187 runs   (    0.62 ms per token,  1608.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1028.13 ms /    71 tokens (   14.48 ms per token,    69.06 tokens per second)\n",
      "llama_print_timings:        eval time =   19605.78 ms /   186 runs   (  105.41 ms per token,     9.49 tokens per second)\n",
      "llama_print_timings:       total time =   21319.62 ms\n"
     ]
    }
   ],
   "source": [
    "# if the chat history is set correctly, the llama 2 should be able to answer St. Stephen's Cathedral (Stephansdom)\n",
    "print(chain.run(text=\"Tell me more about #2.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the trading strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the trading strategies are stored in `code` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the langchain document parser to parse the trading code\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \"../code\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=0), # minimum lines of code to activate parsing, default=0, https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/document_loaders/parsers/language/language_parser.py\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../code/vectorbt-test.py',\n",
       " 'content_type': 'simplified_code',\n",
       " 'language': <Language.PYTHON: 'python'>}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This is a script to calculate simple moving average trading strategy\n",
      "import yfinance as yf\n",
      "import vectorbt as vbt\n",
      "\n",
      "# Fetch historical data from Yahoo Finance\n",
      "ticker_symbol = 'AAPL'  # Apple Inc.\n",
      "data = yf.download(ticker_symbol, start='2020-01-01', end='2023-01-01')\n",
      "\n",
      "# Select only the 'Close' column for simplicity\n",
      "close_prices = data['Close']\n",
      "\n",
      "# Calculate moving averages\n",
      "short_sma = close_prices.rolling(window=10).mean()\n",
      "long_sma = close_prices.rolling(window=50).mean()\n",
      "\n",
      "# Generate signals: where short SMA crosses above long SMA (entry), and below (exit)\n",
      "entries = short_sma > long_sma\n",
      "exits = short_sma <= long_sma\n",
      "\n",
      "# Use vectorbt to simulate trades based on these signals\n",
      "portfolio = vbt.Portfolio.from_signals(close_prices, entries, exits, fees=0.001, freq='1D')\n",
      "\n",
      "# Get and print the performance metrics\n",
      "performance = portfolio.stats()\n",
      "print(performance)\n",
      "\n",
      "# Plot the portfolio and trades\n",
      "portfolio.plot().show()\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the python code in recursive splitter\n",
    "# https://python.langchain.com/docs/modules/data_connection/document_transformers/code_splitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=2000, # maximum of tokens that are parsed in a file, exceeding it will go to next texts\n",
    "    chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Chroma vector store and uses the HuggingFaceEmbeddings to embed the python code into vectors\n",
    "# We uses mpnet-base-v2 here\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(texts, HuggingFaceEmbeddings(cache_folder=\"../model/\", model_kwargs={\"device\":\"cuda:1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we uses mmr as similarity search method and sets nearest neighbor as 1 to only extract one trading strategy\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the prompt template uploaded in langchain prompt hub\n",
    "#https://python.langchain.com/docs/use_cases/code_understanding\n",
    "from langchain import hub\n",
    "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# This is a script to calculate simple moving average trading strategy\\nimport yfinance as yf\\nimport vectorbt as vbt\\n\\n# Fetch historical data from Yahoo Finance\\nticker_symbol = 'AAPL'  # Apple Inc.\\ndata = yf.download(ticker_symbol, start='2020-01-01', end='2023-01-01')\\n\\n# Select only the 'Close' column for simplicity\\nclose_prices = data['Close']\\n\\n# Calculate moving averages\\nshort_sma = close_prices.rolling(window=10).mean()\\nlong_sma = close_prices.rolling(window=50).mean()\\n\\n# Generate signals: where short SMA crosses above long SMA (entry), and below (exit)\\nentries = short_sma > long_sma\\nexits = short_sma <= long_sma\\n\\n# Use vectorbt to simulate trades based on these signals\\nportfolio = vbt.Portfolio.from_signals(close_prices, entries, exits, fees=0.001, freq='1D')\\n\\n# Get and print the performance metrics\\nperformance = portfolio.stats()\\nprint(performance)\\n\\n# Plot the portfolio and trades\\nportfolio.plot().show()\", metadata={'language': 'python', 'source': '../code/vectorbt-test.py'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Could you show me how to use a simple moving average for trading in Google Stocks?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Question-answer chain where it has arguments accepting chat history and documents (i.e., the python code in this case)\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the system prompt to make the LLM aware of its mission to generate trading code\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"\"\"\n",
    "                  You are a helpful assistant in coding Python trading strategy. \n",
    "                  When you write python code, please enclose your python block by ```python ...your python code ```. \n",
    "                  Thanks\n",
    "                  \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Here is my codebase {context} \\n Question: {text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"text\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt_template, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "                  You are a helpful assistant in coding Python trading strategy. \n",
      "                  When you write python code, please enclose your python block by ```python ...your python code ```. \n",
      "                  Thanks\n",
      "                  \n",
      "Human: Here is my codebase # This is a script to calculate simple moving average trading strategy\n",
      "import yfinance as yf\n",
      "import vectorbt as vbt\n",
      "\n",
      "# Fetch historical data from Yahoo Finance\n",
      "ticker_symbol = 'AAPL'  # Apple Inc.\n",
      "data = yf.download(ticker_symbol, start='2020-01-01', end='2023-01-01')\n",
      "\n",
      "# Select only the 'Close' column for simplicity\n",
      "close_prices = data['Close']\n",
      "\n",
      "# Calculate moving averages\n",
      "short_sma = close_prices.rolling(window=10).mean()\n",
      "long_sma = close_prices.rolling(window=50).mean()\n",
      "\n",
      "# Generate signals: where short SMA crosses above long SMA (entry), and below (exit)\n",
      "entries = short_sma > long_sma\n",
      "exits = short_sma <= long_sma\n",
      "\n",
      "# Use vectorbt to simulate trades based on these signals\n",
      "portfolio = vbt.Portfolio.from_signals(close_prices, entries, exits, fees=0.001, freq='1D')\n",
      "\n",
      "# Get and print the performance metrics\n",
      "performance = portfolio.stats()\n",
      "print(performance)\n",
      "\n",
      "# Plot the portfolio and trades\n",
      "portfolio.plot().show() \n",
      " Question: Could you show me how to use a simple moving average for trading in Google Stocks?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Please help me with this.\n",
      "\n",
      "Answer: Sure, I can definitely help you with that! Here's an example of how you could use a simple moving average (SMA) to trade stocks on Google using Python and the yfinance library:\n",
      "```python\n",
      "# Import libraries\n",
      "import yfinance as yf\n",
      "\n",
      "# Set the ticker symbol for the stock you want to trade\n",
      "ticker_symbol = 'GOOG'  # Google Inc.\n",
      "\n",
      "# Fetch historical data from Yahoo Finance\n",
      "data = yf.download(ticker_symbol, start='2020-01-01', end='2023-01-01')\n",
      "\n",
      "# Select only the 'Close' column for simplicity\n",
      "close_prices = data['Close']\n",
      "\n",
      "# Calculate moving averages\n",
      "short_sma = close_prices.rolling(window=10).mean()\n",
      "long_sma = close_prices.rolling(window=50).mean()\n",
      "\n",
      "# Generate signals: where short SMA crosses above long SMA (entry), and below (exit)\n",
      "entries = short_sma > long_sma\n",
      "exits = short_sma <= long_sma\n",
      "\n",
      "# Create a portfolio and add trades based on the signals\n",
      "portfolio = yf.Portfolio(ticker_symbol)\n",
      "for entry, exit in zip(entries, exits):\n",
      "    if entry:\n",
      "        portfolio.buy(size=100)\n",
      "    else:\n",
      "        portfolio.sell()\n",
      "\n",
      "# Get and print the performance metrics\n",
      "performance = portfolio.stats()\n",
      "print(performance)\n",
      "\n",
      "# Plot the portfolio and trades\n",
      "portfolio.plot().show()\n",
      "```\n",
      "This code will fetch historical data for Google stock from Yahoo Finance, calculate simple moving averages with windows of 10 and 50 days, and use the signals generated by the short SMA crossing above/below the long SMA to buy/sell the stock. The portfolio and trades are stored in a `yfinance.Portfolio` object, and the performance metrics and plot are printed/shown using the `stats()` and `plot()` methods.\n",
      "\n",
      "Note that this is just a simple example, and you may want to adjust the window sizes and other parameters based on your own trading strategy and risk tolerance. Additionally, you should always do your own research and consult with a financial advisor before making any investment decisions.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     964.13 ms\n",
      "llama_print_timings:      sample time =     323.99 ms /   552 runs   (    0.59 ms per token,  1703.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5874.72 ms /   394 tokens (   14.91 ms per token,    67.07 tokens per second)\n",
      "llama_print_timings:        eval time =   79942.48 ms /   551 runs   (  145.09 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:       total time =   89270.30 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_text': \" \\n\\nPlease help me with this.\\n\\nAnswer: Sure, I can definitely help you with that! Here's an example of how you could use a simple moving average (SMA) to trade stocks on Google using Python and the yfinance library:\\n```python\\n# Import libraries\\nimport yfinance as yf\\n\\n# Set the ticker symbol for the stock you want to trade\\nticker_symbol = 'GOOG'  # Google Inc.\\n\\n# Fetch historical data from Yahoo Finance\\ndata = yf.download(ticker_symbol, start='2020-01-01', end='2023-01-01')\\n\\n# Select only the 'Close' column for simplicity\\nclose_prices = data['Close']\\n\\n# Calculate moving averages\\nshort_sma = close_prices.rolling(window=10).mean()\\nlong_sma = close_prices.rolling(window=50).mean()\\n\\n# Generate signals: where short SMA crosses above long SMA (entry), and below (exit)\\nentries = short_sma > long_sma\\nexits = short_sma <= long_sma\\n\\n# Create a portfolio and add trades based on the signals\\nportfolio = yf.Portfolio(ticker_symbol)\\nfor entry, exit in zip(entries, exits):\\n    if entry:\\n        portfolio.buy(size=100)\\n    else:\\n        portfolio.sell()\\n\\n# Get and print the performance metrics\\nperformance = portfolio.stats()\\nprint(performance)\\n\\n# Plot the portfolio and trades\\nportfolio.plot().show()\\n```\\nThis code will fetch historical data for Google stock from Yahoo Finance, calculate simple moving averages with windows of 10 and 50 days, and use the signals generated by the short SMA crossing above/below the long SMA to buy/sell the stock. The portfolio and trades are stored in a `yfinance.Portfolio` object, and the performance metrics and plot are printed/shown using the `stats()` and `plot()` methods.\\n\\nNote that this is just a simple example, and you may want to adjust the window sizes and other parameters based on your own trading strategy and risk tolerance. Additionally, you should always do your own research and consult with a financial advisor before making any investment decisions.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\":docs, \"text\": question}, return_only_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fite7001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5\n",
      "  Device 1: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from ../model/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  107.56 MiB\n",
      "llm_load_tensors: VRAM used           = 8694.21 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4092\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 3196.88 MiB, K (f16): 1598.44 MiB, V (f16): 1598.44 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 2861.86 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 2858.67 MiB\n",
      "llama_new_context_with_model: total VRAM used: 11552.88 MiB (model: 8694.21 MiB, context: 2858.67 MiB)\n"
     ]
    }
   ],
   "source": [
    "# import langchain and llamacpp\n",
    "# llama cpp is a C++ interface that enables LLM to run locally with GPU support\n",
    "# we should initialize a llamacpp instance first and later langchain provides a wrapper for it\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../model/llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    n_ctx=4092, # context of token size\n",
    "    n_gpu_layers=-1, #setting -1 to offload all the LLM layers to all available gpu \n",
    "    n_batch=4096, # no. of token in a prompt fed in LLM each time in a batch\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=False,\n",
    "    echo=True,\n",
    "    max_tokens=2048 # max tokens LLM could generate/answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''<s>[INST] <<SYS>>\n",
    "Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to trigger actions for User by responding with JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "Actions available to Assistant are:\n",
    "\n",
    "- \"StockExecutionTool\": Useful for when you need to execute stock trading orders for users.\n",
    "  - To use the StockExecutionTool tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"action\": \"StockExecutionTool\",\n",
    "      \"action_input\": \"('symbol', 'side', 'quantity')\"}}\n",
    "    ```\n",
    "  * `symbol` is the string of stock symbol for trading\n",
    "  * `side` should be enum of string where \"buy\" to buy the stork or \"sell\" to sell the stock\n",
    "  * `quantity` is the float number of quantity of stock to trade\n",
    "- \"ViewAccountTool\": Useful for when you need to query the trading account status such as buying power, and Profit and Loss\n",
    "  - To use the ViewAccountTool tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"action\": \"ViewAccountTool\",\n",
    "      \"action_input\": 'view_action'}}\n",
    "    * the view_action is an enum of string where it could be 'buying_power' if you want to view the buying power of the account or 'PnL' if you want to view the Profit or Loss\n",
    "    ```\n",
    "- \"ViewPositionTool\": Useful for when you need to query the position of individual stocks or all the stocks\n",
    "  - To use the ViewAccountTool tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"action\": \"ViewPositionTool\",\n",
    "      \"action_input\": 'symbol'}}\n",
    "  * `symbol` is the string of stock symbol to view the position, in case the user want to query all the position, please return \"ALL_STOCK\" as symbol \n",
    "    ```\n",
    "\n",
    "Here are some sample conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
    "```\n",
    "User: I'm great, could you help buy 1 Apple Stock?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"StockExecutionTool\",\n",
    " \"action_input\": \"('AAPL', 'buy', '1')\"}}\n",
    "```\n",
    "User: Could I view my buying power of my trading account?\n",
    "Assistant: ```json\n",
    "{{\"action\": \"ViewAccountTool\",\n",
    " \"action_input\": \"buying_power\"}}\n",
    "```\n",
    "\n",
    "User: I want to know the profit and loss of my account\n",
    "Assistant: ```json\n",
    "{{\"action\": \"ViewAccountTool\",\n",
    " \"action_input\": \"PnL\"}}\n",
    "```\n",
    "\n",
    "You are only allowed to return ```json {{\"action\": ..., \"action_input\"}} ``` .\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{0}[/INST]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.trading.client import TradingClient\n",
    "from alpaca.trading.requests import MarketOrderRequest\n",
    "from alpaca.trading.enums import OrderSide, TimeInForce\n",
    "import json\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config=dotenv_values(\"../.env\")\n",
    "\n",
    "trading_client = TradingClient(config['ALPACA_API_KEY'], config['ALPACA_SECRET_KEY'])\n",
    "\n",
    "def execute_order(symbol, side, quantity):\n",
    "    \"\"\"Execute stock trading orders based on the stock symbol, position side, and quantity\"\"\"\n",
    "\n",
    "    # preparing market order\n",
    "    market_order_data = MarketOrderRequest(\n",
    "                        symbol=symbol,\n",
    "                        qty=quantity,\n",
    "                        side=side,\n",
    "                        time_in_force=TimeInForce.DAY\n",
    "                        )\n",
    "    # Market order\n",
    "    market_order = trading_client.submit_order(\n",
    "                    order_data=market_order_data\n",
    "                )\n",
    "    return {\"order_id\": market_order.client_order_id, \"order_status\": market_order.status}\n",
    "\n",
    "def view_account_status(info_type):\n",
    "    account = trading_client.get_account()\n",
    "    if info_type == 'buying_power':\n",
    "        return {'response': f'${account.buying_power} is available as buying power.' }\n",
    "    if info_type == 'PnL':\n",
    "        balance_change = float(account.equity) - float(account.last_equity)\n",
    "        return {'response': f'Today\\'s portfolio balance change: ${balance_change}' }\n",
    "def get_position(symbol):\n",
    "    if symbol == \"ALL_STOCK\":\n",
    "        portfolio = trading_client.get_all_positions()\n",
    "        # Print the quantity of shares for each position.\n",
    "        positions = {}\n",
    "        for position in portfolio:\n",
    "            position = json.loads(position.model_dump_json())\n",
    "            positions[position['symbol']] = {key: position[key] for key in ['qty', 'side', 'cost_basis', 'market_value']}\n",
    "        return {\"response\": positions}\n",
    "    else:\n",
    "        position = json.loads(trading_client.get_open_position(symbol).model_dump_json())\n",
    "        selected_attributes = {key: position[key] for key in ['symbol', 'qty', 'side', 'cost_basis', 'market_value']}\n",
    "        return {\"response\": selected_attributes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_command(command):\n",
    "    # Put user command into prompt (in future projects we'll be re-injecting whole chat history here)\n",
    "    prompt = prompt_template.format(\"User: \" + command)\n",
    "    \n",
    "    # Send command to the model\n",
    "    # Assuming `llm.invoke` is a placeholder for sending command to an LLM (Language Learning Model)\n",
    "    # and getting a response back\n",
    "    output = llm.invoke(prompt, stop=[\"User:\"])\n",
    "    response = output\n",
    "\n",
    "    # Initialize result outside of try-except block\n",
    "    result = None\n",
    "    \n",
    "    # Try to process the response and perform actions\n",
    "    try:\n",
    "        # Extract json from model response by finding first and last brackets {}\n",
    "        firstBracketIndex = response.index(\"{\")\n",
    "        lastBracketIndex = len(response) - response[::-1].index(\"}\")\n",
    "        jsonString = response[firstBracketIndex:lastBracketIndex]\n",
    "        responseJson = json.loads(jsonString)\n",
    "        \n",
    "        if responseJson['action'] == 'StockExecutionTool':\n",
    "            action_input = eval(responseJson['action_input'])\n",
    "            result = execute_order(action_input[0], action_input[1], float(action_input[2]))\n",
    "        elif responseJson['action'] == 'ViewAccountTool':\n",
    "            action_input = responseJson['action_input']\n",
    "            result = view_account_status(action_input)\n",
    "        elif responseJson['action'] == 'ViewPositionTool':\n",
    "            action_input = responseJson['action_input']\n",
    "            result = get_position(action_input)\n",
    "            \n",
    "        # If result was computed, yield it\n",
    "        if result:\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        final_response = {\"response\": \"The action is not triggered. Please try again.\"}\n",
    "        return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's my response:\n",
      "\n",
      "{\"action\": \"StockExecutionTool\",\n",
      " \"action_input\": \"('GOOG', 'buy', '10')\"}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'order_id': '775985db-ade8-424f-9dbc-cf8479978fed',\n",
       " 'order_status': <OrderStatus.ACCEPTED: 'accepted'>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"Can you help buy me 10 Google Stock?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I'd be happy to help! Here's the response from the Assistant:\n",
      "\n",
      "{\"action\": \"ViewAccountTool\", \"action_input\": \"buying_power\"}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': '$135948.91 is available as buying power.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"what is my current account buying power. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's my response as the Assistant:\n",
      "\n",
      "{\"action\": \"ViewAccountTool\", \"action_input\": \"PnL\"}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': \"Today's portfolio balance change: $-0.0012199999910080805\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"I want to know my current PnL Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's the response from the Assistant:\n",
      "\n",
      "{\"action\": \"ViewPositionTool\", \"action_input\": \"AAPL\"}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': {'symbol': 'AAPL',\n",
       "  'qty': '1',\n",
       "  'side': 'long',\n",
       "  'cost_basis': '170.355',\n",
       "  'market_value': '171.48'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"I want to know my position of Apple Stock.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I can help you with that! Here's the information you requested:\n",
      "\n",
      "{\"action\": \"ViewPositionTool\", \"action_input\": \"ALL_STOCK\"}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': {'AAPL': {'qty': '1',\n",
       "   'side': 'long',\n",
       "   'cost_basis': '170.355',\n",
       "   'market_value': '171.48'},\n",
       "  'GOOG': {'qty': '80',\n",
       "   'side': 'long',\n",
       "   'cost_basis': '12104.1',\n",
       "   'market_value': '12180.8'},\n",
       "  'SPY': {'qty': '13.046',\n",
       "   'side': 'long',\n",
       "   'cost_basis': '6798.001259994',\n",
       "   'market_value': '6823.97122'}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_command(\"I want to know my position of all my portfolio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from typing import AsyncGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = {}\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):    \n",
    "    # Make sure the model path is correct for your system!\n",
    "    llms[\"llama\"] = LlamaCpp(\n",
    "        model_path=\"../model/llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "        n_ctx=4092, # context of token size\n",
    "        n_gpu_layers=-1, #setting -1 to offload all the LLM layers to all available gpu \n",
    "        n_batch=4096, # no. of token in a prompt fed in LLM each time in a batch\n",
    "        f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "        verbose=False,\n",
    "        max_tokens=2048 # max tokens LLM could generate/answer\n",
    "    )\n",
    "    yield  \n",
    "\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "\n",
    "def run_llm(question: str) -> AsyncGenerator:\n",
    "    llm : LlamaCPP = llms[\"llama\"]\n",
    "    response_iter = llm.stream_complete(question)\n",
    "    for response in response_iter:\n",
    "        yield f\"data: {response.delta}\\n\\n\"\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root(question: str) -> StreamingResponse:\n",
    "    return StreamingResponse(run_llm(question), media_type=\"text/event-stream\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fite7001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
